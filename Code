# Import essential libraries
import torch
from torch import nn
import matplotlib.pyplot as plt

print("Environment set up.")

# Define a basic neural network layer (example)
class SimpleModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(SimpleModel, self).__init__()
        self.layer = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return self.layer(x)

# Tokenization logic demonstration
text = "The quick brown fox jumps over the lazy dog"
tokens = text.split()
print("Tokens:", tokens)

import torch.nn.functional as F

embedding = nn.Embedding(1000, 32)
input = torch.LongTensor([1, 2, 3, 4])
output = embedding(input)
print("Embedding output shape:", output.shape)

import math

def get_positional_encoding(seq_len, dim):
    encoding = torch.zeros(seq_len, dim)
    for pos in range(seq_len):
        for i in range(0, dim, 2):
            encoding[pos, i] = math.sin(pos / (10000 ** ((2 * i)/dim)))
            if i + 1 < dim:
                encoding[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/dim)))
    return encoding

# Generate positional encoding
encoding = get_positional_encoding(50, 16)
print("Generated positional encoding shape:", encoding.shape)

from torch.nn import Transformer

transformer_model = Transformer(d_model=512, nhead=8, num_encoder_layers=2)
print("Initialized a basic Transformer model.")




